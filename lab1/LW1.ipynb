{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30f46f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0efa8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = \"train.csv\"\n",
    "test_csv  = \"test.csv\"\n",
    "TARGET_COLUMN = 'RiskScore'\n",
    "\n",
    "abs_cap = 900.0\n",
    "top_k = 12\n",
    "degree = 3\n",
    "clip_val = 13.0\n",
    "eps = 1e-21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64caae9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Этап 1: Загрузка и предварительная обработка данных ---\n",
      "Исходный размер обучающей выборки: (10272, 34)\n",
      "Исходная предобработка завершена. df_processed инициализирован.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Этап 1: Загрузка и предварительная обработка данных ---\")\n",
    "\n",
    "df = pd.read_csv(train_csv)\n",
    "\n",
    "df[TARGET_COLUMN] = pd.to_numeric(df[TARGET_COLUMN])\n",
    "df = df.dropna(subset=[TARGET_COLUMN])\n",
    "df = df[np.isfinite(df[TARGET_COLUMN])]\n",
    "\n",
    "if abs_cap is not None:\n",
    "    df = df[df[TARGET_COLUMN].abs() <= abs_cap]\n",
    "\n",
    "df[TARGET_COLUMN] = np.clip(df[TARGET_COLUMN], -200, 200)\n",
    "\n",
    "if 'ID' in df.columns:\n",
    "    df = df.drop(columns=['ID'])\n",
    "if 'ApplicationDate' in df.columns:\n",
    "    df = df.drop(columns=['ApplicationDate'])\n",
    "\n",
    "print(\"Исходный размер обучающей выборки:\", df.shape)\n",
    "\n",
    "cat_cols = [\n",
    "    \"MaritalStatus\",\n",
    "    \"HomeOwnershipStatus\",\n",
    "    \"LoanPurpose\",\n",
    "    \"EmploymentStatus\",\n",
    "    \"EducationLevel\",\n",
    "]\n",
    "\n",
    "X_numeric_initial = df.select_dtypes(include=[np.number]).drop(columns=[TARGET_COLUMN], errors='ignore')\n",
    "X_categorical_initial = df[cat_cols].copy()\n",
    "\n",
    "X_numeric_initial = X_numeric_initial.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "lower_q, upper_q = 0.01, 0.99\n",
    "lower_bounds = X_numeric_initial.quantile(lower_q)\n",
    "upper_bounds = X_numeric_initial.quantile(upper_q)\n",
    "\n",
    "X_winsorized = X_numeric_initial.copy()\n",
    "for col in X_winsorized.columns:\n",
    "    x = X_winsorized[col].to_numpy(dtype=float, copy=True)\n",
    "    lo = lower_bounds.get(col, np.nan)\n",
    "    hi = upper_bounds.get(col, np.nan)\n",
    "    if np.isfinite(lo):\n",
    "        x = np.where(np.isfinite(x), np.maximum(x, lo), x)\n",
    "    if np.isfinite(hi):\n",
    "        x = np.where(np.isfinite(x), np.minimum(x, hi), x)\n",
    "    X_winsorized[col] = x\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\").fit(X_winsorized)\n",
    "X_imputed = pd.DataFrame(\n",
    "    imputer.transform(X_winsorized),\n",
    "    columns=X_winsorized.columns,\n",
    "    index=X_winsorized.index,\n",
    ")\n",
    "\n",
    "scaler = StandardScaler().fit(X_imputed)\n",
    "X_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_imputed),\n",
    "    columns=X_imputed.columns,\n",
    "    index=X_imputed.index,\n",
    ")\n",
    "\n",
    "label_encoders = {}\n",
    "for column in X_categorical_initial.select_dtypes(include=['object']).columns.tolist():\n",
    "    le = LabelEncoder()\n",
    "    all_categories = X_categorical_initial[column].astype(str).unique()\n",
    "    le.fit(all_categories)\n",
    "    X_categorical_initial[column] = le.transform(X_categorical_initial[column].astype(str))\n",
    "    label_encoders[column] = le\n",
    "\n",
    "df_processed = pd.concat([X_scaled, X_categorical_initial], axis=1)\n",
    "df_processed[TARGET_COLUMN] = df[TARGET_COLUMN]\n",
    "\n",
    "print(\"Исходная предобработка завершена. df_processed инициализирован.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b4fcfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Этап 2: Создание новых признаков ---\n",
      "Новые признаки добавлены.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Этап 2: Создание новых признаков ---\")\n",
    "\n",
    "def create_financial_stability_features(df_input):\n",
    "    df_enhanced = df_input.copy()\n",
    "    required_cols = ['NetWorth', 'TotalLiabilities', 'TotalAssets', 'AnnualIncome',\n",
    "                     'SavingsAccountBalance', 'CheckingAccountBalance', 'MonthlyDebtPayments',\n",
    "                     'MonthlyLoanPayment', 'MonthlyIncome']\n",
    "    if not all(col in df_enhanced.columns for col in required_cols):\n",
    "        return df_enhanced\n",
    "    df_enhanced['FinancialIndependenceRatio'] = (df_enhanced['NetWorth'] + 1) / (df_enhanced['TotalLiabilities'] + 1)\n",
    "    df_enhanced['AssetDebtCoverage'] = df_enhanced['TotalAssets'] / (df_enhanced['TotalLiabilities'] + 1)\n",
    "    df_enhanced['FinancialCushion'] = (df_enhanced['SavingsAccountBalance'] + df_enhanced['CheckingAccountBalance']) / (df_enhanced['AnnualIncome'] / 12 + 1)\n",
    "    df_enhanced['MonthsOfSavings'] = (df_enhanced['SavingsAccountBalance'] + df_enhanced['CheckingAccountBalance']) / (df_enhanced['MonthlyDebtPayments'] + 1)\n",
    "    df_enhanced['LiquidityRisk'] = (df_enhanced['MonthlyDebtPayments'] + df_enhanced['MonthlyLoanPayment']) / (df_enhanced['MonthlyIncome'] + 1)\n",
    "    df_enhanced['NetCashFlow'] = df_enhanced['MonthlyIncome'] - df_enhanced['MonthlyDebtPayments'] - df_enhanced['MonthlyLoanPayment']\n",
    "    df_enhanced['CashFlowAdequacy'] = df_enhanced['NetCashFlow'] / (df_enhanced['MonthlyLoanPayment'] + 1)\n",
    "    return df_enhanced\n",
    "\n",
    "def create_debt_risk_features(df_input):\n",
    "    df_enhanced = df_input.copy()\n",
    "    required_cols = ['MonthlyDebtPayments', 'MonthlyLoanPayment', 'MonthlyIncome',\n",
    "                     'TotalLiabilities', 'TotalAssets', 'CreditScore', 'DebtToIncomeRatio',\n",
    "                     'LoanAmount', 'AnnualIncome']\n",
    "    if not all(col in df_enhanced.columns for col in required_cols):\n",
    "        return df_enhanced\n",
    "    df_enhanced['TotalDebtBurden'] = df_enhanced['MonthlyDebtPayments'] + df_enhanced['MonthlyLoanPayment']\n",
    "    df_enhanced['TotalDebtToIncome'] = df_enhanced['TotalDebtBurden'] / (df_enhanced['MonthlyIncome'] + 1)\n",
    "    df_enhanced['DebtTrapRisk'] = df_enhanced['TotalLiabilities'] / (df_enhanced['TotalAssets'] + 1)\n",
    "    df_enhanced['DefaultRiskScore'] = ((850 - df_enhanced['CreditScore']) / 850) * df_enhanced['DebtToIncomeRatio']\n",
    "    df_enhanced['NewLoanBurden'] = df_enhanced['MonthlyLoanPayment'] / (df_enhanced['MonthlyIncome'] + 1)\n",
    "    df_enhanced['LoanToIncomeAnnual'] = df_enhanced['LoanAmount'] / (df_enhanced['AnnualIncome'] + 1)\n",
    "    return df_enhanced\n",
    "\n",
    "def add_comprehensive_binning_features(df_input):\n",
    "    df_enhanced = df_input.copy()\n",
    "    potential_binning_cols = ['DebtToIncomeRatio', 'LoanAmount', 'AnnualIncome', 'SavingsAccountBalance',\n",
    "                              'TotalAssets', 'TotalLiabilities', 'NetWorth', 'CreditCardUtilizationRate',\n",
    "                              'NumberOfOpenCreditLines', 'NumberOfCreditInquiries', 'LengthOfCreditHistory',\n",
    "                              'PaymentHistory', 'MonthlyIncome', 'CreditScore']\n",
    "\n",
    "    features_for_binning = {}\n",
    "    for col in potential_binning_cols:\n",
    "        if col in df_enhanced.columns:\n",
    "            features_for_binning[col] = df_enhanced[col]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    if not features_for_binning:\n",
    "        return df_enhanced\n",
    "\n",
    "    for col in ['AnnualIncome', 'MonthlyIncome', 'TotalLiabilities', 'TotalAssets', 'LoanAmount', 'DebtToIncomeRatio', 'SavingsAccountBalance', 'NetWorth', 'CreditCardUtilizationRate', 'NumberOfOpenCreditLines', 'NumberOfCreditInquiries', 'LengthOfCreditHistory', 'PaymentHistory']:\n",
    "        if col not in df_enhanced.columns: df_enhanced[col] = 0\n",
    "\n",
    "    if 'DebtToIncomeRatio' in df_enhanced.columns:\n",
    "        df_enhanced['DebtToIncome_Binned'] = pd.cut(df_enhanced['DebtToIncomeRatio'], bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0], labels=[1, 2, 3, 4, 5]).astype(float)\n",
    "    else:\n",
    "        df_enhanced['DebtToIncome_Binned'] = np.nan\n",
    "\n",
    "    if 'AnnualIncome' in df_enhanced.columns:\n",
    "        loan_to_income = df_enhanced['LoanAmount'] / (df_enhanced['AnnualIncome'] + 1) if 'LoanAmount' in df_enhanced.columns else 0\n",
    "        df_enhanced['LoanToIncome_Binned'] = pd.cut(loan_to_income, bins=[0, 0.5, 1.0, 1.5, 2.0, 10], labels=[1, 2, 3, 4, 5]).astype(float).fillna(3)\n",
    "    else:\n",
    "        df_enhanced['LoanToIncome_Binned'] = np.nan\n",
    "\n",
    "    if 'AnnualIncome' in df_enhanced.columns and 'SavingsAccountBalance' in df_enhanced.columns:\n",
    "        savings_ratio = df_enhanced['SavingsAccountBalance'] / (df_enhanced['AnnualIncome'] + 1)\n",
    "        df_enhanced['SavingsRatio_Binned'] = pd.cut(savings_ratio, bins=[0, 0.1, 0.25, 0.5, 1.0, 100], labels=[5, 4, 3, 2, 1]).astype(float).fillna(3)\n",
    "    else:\n",
    "        df_enhanced['SavingsRatio_Binned'] = np.nan\n",
    "\n",
    "    if 'TotalAssets' in df_enhanced.columns and 'TotalLiabilities' in df_enhanced.columns:\n",
    "        asset_coverage = df_enhanced['TotalAssets'] / (df_enhanced['TotalLiabilities'] + 1)\n",
    "        df_enhanced['AssetCoverage_Binned'] = pd.cut(asset_coverage, bins=[0, 0.5, 1.0, 2.0, 5.0, 1000], labels=[5, 4, 3, 2, 1]).astype(float).fillna(3)\n",
    "    else:\n",
    "        df_enhanced['AssetCoverage_Binned'] = np.nan\n",
    "\n",
    "    if 'AnnualIncome' in df_enhanced.columns and 'NetWorth' in df_enhanced.columns:\n",
    "        networth_ratio = df_enhanced['NetWorth'] / (df_enhanced['AnnualIncome'] + 1)\n",
    "        df_enhanced['NetWorthRatio_Binned'] = pd.cut(networth_ratio, bins=[-1000, 0, 0.5, 1.0, 2.0, 1000], labels=[5, 4, 3, 2, 1]).astype(float).fillna(3)\n",
    "    else:\n",
    "        df_enhanced['NetWorthRatio_Binned'] = np.nan\n",
    "\n",
    "    if 'CreditCardUtilizationRate' in df_enhanced.columns:\n",
    "        df_enhanced['CreditUtilization_Binned'] = pd.cut(df_enhanced['CreditCardUtilizationRate'], bins=[0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0], labels=[1, 2, 3, 4, 5, 6]).astype(float).fillna(3)\n",
    "    else:\n",
    "        df_enhanced['CreditUtilization_Binned'] = np.nan\n",
    "\n",
    "    if 'NumberOfOpenCreditLines' in df_enhanced.columns:\n",
    "        df_enhanced['CreditLines_Binned'] = pd.cut(df_enhanced['NumberOfOpenCreditLines'], bins=[0, 1, 3, 5, 7, 10, 100], labels=[1, 2, 3, 4, 5, 6]).astype(float).fillna(3)\n",
    "    else:\n",
    "        df_enhanced['CreditLines_Binned'] = np.nan\n",
    "\n",
    "    if 'NumberOfCreditInquiries' in df_enhanced.columns:\n",
    "        df_enhanced['CreditInquiries_Binned'] = pd.cut(df_enhanced['NumberOfCreditInquiries'], bins=[-1, 0, 1, 2, 3, 5, 100], labels=[1, 2, 3, 4, 5, 6]).astype(float).fillna(2)\n",
    "    else:\n",
    "        df_enhanced['CreditInquiries_Binned'] = np.nan\n",
    "\n",
    "    if 'LengthOfCreditHistory' in df_enhanced.columns:\n",
    "        df_enhanced['CreditHistoryLength_Binned'] = pd.cut(df_enhanced['LengthOfCreditHistory'], bins=[0, 2, 5, 7, 10, 15, 100], labels=[6, 5, 4, 3, 2, 1]).astype(float).fillna(4)\n",
    "    else:\n",
    "        df_enhanced['CreditHistoryLength_Binned'] = np.nan\n",
    "\n",
    "    if 'PaymentHistory' in df_enhanced.columns and 'LengthOfCreditHistory' in df_enhanced.columns:\n",
    "        payment_quality = df_enhanced['PaymentHistory'] / (df_enhanced['LengthOfCreditHistory'] + 1)\n",
    "        df_enhanced['PaymentQuality_Binned'] = pd.cut(payment_quality, bins=[0, 0.5, 0.7, 0.85, 0.95, 1.0], labels=[5, 4, 3, 2, 1]).astype(float).fillna(3)\n",
    "    else:\n",
    "        df_enhanced['PaymentQuality_Binned'] = np.nan\n",
    "\n",
    "    for col in df_enhanced.columns:\n",
    "        if df_enhanced[col].isnull().any():\n",
    "            df_enhanced[col] = df_enhanced[col].fillna(df_enhanced[col].median() if pd.api.types.is_numeric_dtype(df_enhanced[col]) else 0)\n",
    "\n",
    "    return df_enhanced\n",
    "\n",
    "\n",
    "def add_log_transformations(df_input, cols_to_log):\n",
    "    df_log = df_input.copy()\n",
    "    for col in cols_to_log:\n",
    "        if col in df_log.columns:\n",
    "            if (df_log[col] >= 0).all():\n",
    "                df_log[f'Log_{col}'] = np.log1p(df_log[col])\n",
    "            else:\n",
    "                min_val = df_log[col].min()\n",
    "                shift = abs(min_val) + 1 if min_val < 0 else 1\n",
    "                df_log[f'Log_{col}'] = np.log1p(df_log[col] + shift)\n",
    "    return df_log\n",
    "\n",
    "df_processed = create_financial_stability_features(df_processed)\n",
    "df_processed = create_debt_risk_features(df_processed)\n",
    "df_processed = add_comprehensive_binning_features(df_processed)\n",
    "\n",
    "all_numeric_cols_after_feature_eng = df_processed.select_dtypes(include=np.number).columns.tolist()\n",
    "numeric_cols_for_log = [col for col in all_numeric_cols_after_feature_eng if col != TARGET_COLUMN and col != 'ApplicationDate']\n",
    "\n",
    "df_processed = add_log_transformations(df_processed, numeric_cols_for_log)\n",
    "\n",
    "print(\"Новые признаки добавлены.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "624813e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Этап 3: Создание и отбор признаков ---\n",
      "Топ-12 признаков по корреляции с RiskScore: ['Log_MonthlyIncome', 'Log_AnnualIncome', 'MonthlyIncome', 'AnnualIncome', 'CreditScore', 'Log_InterestRate', 'InterestRate', 'BaseInterestRate', 'Log_CreditScore', 'Log_BaseInterestRate', 'Log_TotalDebtToIncomeRatio', 'TotalDebtToIncomeRatio']\n",
      "Создано 454 полиномиальных признаков.\n",
      "Создано 330 комбинированных признаков.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Этап 3: Создание и отбор признаков ---\")\n",
    "\n",
    "X = df_processed.drop(TARGET_COLUMN, axis=1)\n",
    "y = df_processed[TARGET_COLUMN]\n",
    "\n",
    "X_numeric_for_corr = X.select_dtypes(include=np.number)\n",
    "\n",
    "if not X_numeric_for_corr.empty and TARGET_COLUMN in df_processed.columns:\n",
    "    correlation_with_target = X_numeric_for_corr.corrwith(y).abs().sort_values(ascending=False)\n",
    "    top_features_for_poly = correlation_with_target.head(top_k).index.tolist()\n",
    "    print(f\"Топ-{top_k} признаков по корреляции с {TARGET_COLUMN}: {top_features_for_poly}\")\n",
    "else:\n",
    "    print(f\"Предупреждение: Числовые признаки отсутствуют или целевая колонка '{TARGET_COLUMN}' отсутствует. Топ-признаки для полиномов не определены.\")\n",
    "    top_features_for_poly = []\n",
    "\n",
    "if top_features_for_poly:\n",
    "    existing_top_features = [f for f in top_features_for_poly if f in X.columns]\n",
    "    if len(existing_top_features) > 0:\n",
    "        poly_transformer = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "        poly_transformer.fit(X[existing_top_features])\n",
    "\n",
    "        X_poly = pd.DataFrame(\n",
    "            poly_transformer.transform(X[existing_top_features]),\n",
    "            columns=poly_transformer.get_feature_names_out(existing_top_features),\n",
    "            index=X.index\n",
    "        )\n",
    "        print(f\"Создано {X_poly.shape[1]} полиномиальных признаков.\")\n",
    "    else:\n",
    "        print(\"Предупреждение: Нет существующих признаков для создания полиномов.\")\n",
    "        X_poly = pd.DataFrame(index=X.index)\n",
    "else:\n",
    "    X_poly = pd.DataFrame(index=X.index)\n",
    "\n",
    "clip_limit = clip_val\n",
    "epsilon = eps\n",
    "\n",
    "def make_safe(arr, clip_limit=None, epsilon=1e-6):\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    arr[~np.isfinite(arr)] = 0.0\n",
    "    if clip_limit is not None:\n",
    "        arr = np.clip(arr, -clip_limit, clip_limit)\n",
    "    return arr\n",
    "\n",
    "new_features_list = []\n",
    "feature_names_for_comb = []\n",
    "features_for_combinations = [f for f in top_features_for_poly if f in X.columns]\n",
    "\n",
    "if len(features_for_combinations) >= 2:\n",
    "    for fa, fb in combinations(features_for_combinations, 2):\n",
    "        if fa in X.columns and fb in X.columns:\n",
    "            xa = X[fa].values\n",
    "            xb = X[fb].values\n",
    "\n",
    "            new_features_list.append(make_safe(xa * xb, clip_limit, epsilon))\n",
    "            feature_names_for_comb.append(f\"{fa}__x__{fb}\")\n",
    "\n",
    "            new_features_list.append(make_safe(xa + xb, clip_limit, epsilon))\n",
    "            feature_names_for_comb.append(f\"{fa}__plus__{fb}\")\n",
    "\n",
    "            new_features_list.append(make_safe(xa - xb, clip_limit, epsilon))\n",
    "            feature_names_for_comb.append(f\"{fa}__minus__{fb}\")\n",
    "\n",
    "            new_features_list.append(make_safe(xa / (xb + epsilon), clip_limit, epsilon))\n",
    "            feature_names_for_comb.append(f\"{fa}__div__{fb}\")\n",
    "\n",
    "            new_features_list.append(make_safe(xb / (xa + epsilon), clip_limit, epsilon))\n",
    "            feature_names_for_comb.append(f\"{fb}__div__{fa}\")\n",
    "\n",
    "    X_combined = pd.DataFrame(np.array(new_features_list).T,\n",
    "                              columns=feature_names_for_comb,\n",
    "                              index=X.index)\n",
    "    print(f\"Создано {X_combined.shape[1]} комбинированных признаков.\")\n",
    "else:\n",
    "    X_combined = pd.DataFrame(index=X.index)\n",
    "\n",
    "X_all_features = pd.concat([X, X_poly, X_combined], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "724169cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Этап 4: Отбор признаков ---\n",
      "Удаление 77 признаков с корреляцией < 0.03.\n",
      "Удаление 558 высоко коррелирующих признаков.\n",
      "Финальное количество признаков: 249\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Этап 4: Отбор признаков ---\")\n",
    "\n",
    "low_corr_threshold = 0.03\n",
    "if not X_all_features.select_dtypes(include=np.number).empty and TARGET_COLUMN in df_processed.columns:\n",
    "    X_all_features_numeric = X_all_features.select_dtypes(include=np.number)\n",
    "    correlations = X_all_features_numeric.corrwith(df_processed[TARGET_COLUMN]).abs()\n",
    "    low_corr_features = correlations[correlations < low_corr_threshold].index.tolist()\n",
    "    print(f\"Удаление {len(low_corr_features)} признаков с корреляцией < {low_corr_threshold}.\")\n",
    "    features_to_keep_low_corr = X_all_features_numeric.columns.difference(low_corr_features).tolist()\n",
    "    X_all_features = X_all_features[features_to_keep_low_corr + X_all_features.select_dtypes(exclude=np.number).columns.tolist()]\n",
    "else:\n",
    "    print(f\"Предупреждение: Числовые признаки отсутствуют или целевая колонка '{TARGET_COLUMN}' отсутствует. Пропуск отбора по низкой корреляции.\")\n",
    "\n",
    "\n",
    "def get_highly_correlated_features_to_remove(df, corr_threshold=0.999, target_corr_threshold=0.22, target_column='RiskScore'):\n",
    "    if target_column not in df.columns or df.select_dtypes(include=np.number).empty:\n",
    "        return []\n",
    "\n",
    "    df_numeric = df.select_dtypes(include=np.number)\n",
    "    if df_numeric.empty:\n",
    "        return []\n",
    "\n",
    "    corr_matrix = df_numeric.corr().abs()\n",
    "\n",
    "    if target_column not in corr_matrix.columns:\n",
    "        return []\n",
    "\n",
    "    target_correlations = corr_matrix[target_column].drop(target_column, errors='ignore')\n",
    "\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones_like(corr_matrix, dtype=bool), k=1))\n",
    "    columns_to_remove = set()\n",
    "\n",
    "    for col in upper_triangle.columns:\n",
    "        if col == target_column: continue\n",
    "\n",
    "        col_target_corr_val = target_correlations.get(col, 0.0)\n",
    "        if not isinstance(col_target_corr_val, (int, float)):\n",
    "            col_target_corr_val = 0.0\n",
    "\n",
    "        high_corr_with_col = upper_triangle[col][upper_triangle[col] > corr_threshold]\n",
    "\n",
    "        for correlated_col in high_corr_with_col.index:\n",
    "            if correlated_col == target_column: continue\n",
    "\n",
    "            correlated_col_target_corr_val = target_correlations.get(correlated_col, 0.0)\n",
    "            if not isinstance(correlated_col_target_corr_val, (int, float)):\n",
    "                correlated_col_target_corr_val = 0.0\n",
    "\n",
    "            if col_target_corr_val < target_corr_threshold and correlated_col_target_corr_val < target_corr_threshold:\n",
    "                if col_target_corr_val < correlated_col_target_corr_val:\n",
    "                    columns_to_remove.add(col)\n",
    "                else:\n",
    "                    columns_to_remove.add(correlated_col)\n",
    "            elif col_target_corr_val < target_corr_threshold and correlated_col_target_corr_val >= target_corr_threshold:\n",
    "                columns_to_remove.add(col)\n",
    "            elif col_target_corr_val >= target_corr_threshold and correlated_col_target_corr_val < target_corr_threshold:\n",
    "                columns_to_remove.add(correlated_col)\n",
    "            elif col_target_corr_val >= target_corr_threshold and correlated_col_target_corr_val >= target_corr_threshold:\n",
    "                 if col < correlated_col:\n",
    "                     columns_to_remove.add(col)\n",
    "                 else:\n",
    "                     columns_to_remove.add(correlated_col)\n",
    "    return list(columns_to_remove)\n",
    "    \n",
    "if TARGET_COLUMN in df_processed.columns and pd.api.types.is_numeric_dtype(df_processed[TARGET_COLUMN]):\n",
    "    df_for_high_corr_check = pd.concat([X_all_features, df_processed[[TARGET_COLUMN]]], axis=1)\n",
    "    highly_correlated_to_remove = get_highly_correlated_features_to_remove(df_for_high_corr_check, corr_threshold=0.98)\n",
    "    print(f\"Удаление {len(highly_correlated_to_remove)} высоко коррелирующих признаков.\")\n",
    "    X_all_features = X_all_features.drop(columns=highly_correlated_to_remove, errors='ignore')\n",
    "else:\n",
    "    print(f\"Предупреждение: Целевая колонка '{TARGET_COLUMN}' отсутствует или не является числовой. Пропуск отбора высоко коррелирующих признаков.\")\n",
    "\n",
    "\n",
    "print(f\"Финальное количество признаков: {X_all_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83e8209e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Этап 5: Масштабирование финальных признаков ---\n",
      "Финальные числовые признаки масштабированы StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Этап 5: Масштабирование финальных признаков ---\")\n",
    "X_all_features_numeric = X_all_features.select_dtypes(include=np.number)\n",
    "X_all_features_categorical = X_all_features.select_dtypes(exclude=np.number)\n",
    "\n",
    "if not X_all_features_numeric.empty:\n",
    "    final_scaler = StandardScaler()\n",
    "    X_scaled_final_numeric = pd.DataFrame(\n",
    "        final_scaler.fit_transform(X_all_features_numeric),\n",
    "        columns=X_all_features_numeric.columns,\n",
    "        index=X_all_features_numeric.index\n",
    "    )\n",
    "    X_scaled_final = pd.concat([X_scaled_final_numeric, X_all_features_categorical], axis=1)\n",
    "    print(\"Финальные числовые признаки масштабированы StandardScaler.\")\n",
    "else:\n",
    "    X_scaled_final = X_all_features_categorical\n",
    "    print(\"Нет числовых признаков для финального масштабирования.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a25e786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Этап 6: Подготовка тестовых данных ---\n",
      "Финальные тестовые числовые признаки масштабированы StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Этап 6: Подготовка тестовых данных ---\")\n",
    "df_test = pd.read_csv(test_csv)\n",
    "\n",
    "test_ids = None\n",
    "if 'ID' in df_test.columns:\n",
    "    test_ids = df_test['ID'].copy()\n",
    "    df_test = df_test.drop(columns=['ID'])\n",
    "if 'ApplicationDate' in df_test.columns:\n",
    "    df_test = df_test.drop(columns=['ApplicationDate'])\n",
    "\n",
    "df_test = df_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "X_numeric_test_initial = df_test.select_dtypes(include=[np.number])\n",
    "X_categorical_test_initial = df_test[cat_cols].copy()\n",
    "\n",
    "X_winsorized_test = X_numeric_test_initial.copy()\n",
    "for col in X_winsorized_test.columns:\n",
    "    x = X_winsorized_test[col].to_numpy(dtype=float, copy=True)\n",
    "    lo = lower_bounds.get(col, np.nan)\n",
    "    hi = upper_bounds.get(col, np.nan)\n",
    "    if np.isfinite(lo):\n",
    "        x = np.where(np.isfinite(x), np.maximum(x, lo), x)\n",
    "    if np.isfinite(hi):\n",
    "        x = np.where(np.isfinite(x), np.minimum(x, hi), x)\n",
    "    X_winsorized_test[col] = x\n",
    "\n",
    "X_imputed_test = pd.DataFrame(\n",
    "    imputer.transform(X_winsorized_test),\n",
    "    columns=X_winsorized_test.columns,\n",
    "    index=X_winsorized_test.index,\n",
    ")\n",
    "\n",
    "X_scaled_test = pd.DataFrame(\n",
    "    scaler.transform(X_imputed_test),\n",
    "    columns=X_imputed_test.columns,\n",
    "    index=X_imputed_test.index,\n",
    ")\n",
    "\n",
    "for column in X_categorical_test_initial.columns.tolist():\n",
    "    if column in label_encoders:\n",
    "        le = label_encoders[column]\n",
    "        test_column_str = X_categorical_test_initial[column].astype(str)\n",
    "        \n",
    "        unknown_mask = ~np.isin(test_column_str, le.classes_)\n",
    "        \n",
    "        if unknown_mask.any():\n",
    "            processed_column = le.transform(test_column_str)\n",
    "            processed_column[unknown_mask] = -1\n",
    "            X_categorical_test_initial[column] = processed_column\n",
    "        else:\n",
    "            X_categorical_test_initial[column] = le.transform(test_column_str)\n",
    "\n",
    "X_test_processed = pd.concat([X_scaled_test, X_categorical_test_initial], axis=1)\n",
    "\n",
    "df_test_processed = create_financial_stability_features(X_test_processed)\n",
    "df_test_processed = create_debt_risk_features(df_test_processed)\n",
    "df_test_processed = add_comprehensive_binning_features(df_test_processed)\n",
    "\n",
    "df_test_processed = add_log_transformations(df_test_processed, numeric_cols_for_log)\n",
    "\n",
    "if top_features_for_poly and all(col in df_test_processed.columns for col in top_features_for_poly):\n",
    "    if 'poly_transformer' in locals() and poly_transformer is not None:\n",
    "        X_test_poly = pd.DataFrame(\n",
    "            poly_transformer.transform(df_test_processed[top_features_for_poly]),\n",
    "            columns=poly_transformer.get_feature_names_out(top_features_for_poly),\n",
    "            index=df_test_processed.index\n",
    "        )\n",
    "    else:\n",
    "        X_test_poly = pd.DataFrame(index=df_test_processed.index)\n",
    "else:\n",
    "    X_test_poly = pd.DataFrame(index=df_test_processed.index)\n",
    "\n",
    "X_test_combined = pd.DataFrame(index=df_test_processed.index)\n",
    "if len(features_for_combinations) >= 2:\n",
    "    new_features_test_list = []\n",
    "    for fa, fb in combinations(features_for_combinations, 2):\n",
    "        if fa in df_test_processed.columns and fb in df_test_processed.columns:\n",
    "            xa_test = df_test_processed[fa].values\n",
    "            xb_test = df_test_processed[fb].values\n",
    "            new_features_test_list.append(make_safe(xa_test * xb_test, clip_limit, epsilon))\n",
    "            new_features_test_list.append(make_safe(xa_test + xb_test, clip_limit, epsilon))\n",
    "            new_features_test_list.append(make_safe(xa_test - xb_test, clip_limit, epsilon))\n",
    "            new_features_test_list.append(make_safe(xa_test / (xb_test + epsilon), clip_limit, epsilon))\n",
    "            new_features_test_list.append(make_safe(xb_test / (xa_test + epsilon), clip_limit, epsilon))\n",
    "        else:\n",
    "            new_features_test_list.extend([np.zeros(len(df_test_processed)) for _ in range(5)])\n",
    "\n",
    "    if new_features_test_list:\n",
    "        X_test_combined = pd.DataFrame(np.array(new_features_test_list).T,\n",
    "                                       columns=feature_names_for_comb,\n",
    "                                       index=df_test_processed.index)\n",
    "    else:\n",
    "        X_test_combined = pd.DataFrame(index=df_test_processed.index)\n",
    "\n",
    "X_test_all_features = pd.concat([df_test_processed, X_test_poly, X_test_combined], axis=1)\n",
    "\n",
    "missing_cols_in_test = set(X_all_features.columns) - set(X_test_all_features.columns)\n",
    "for c in missing_cols_in_test:\n",
    "    X_test_all_features[c] = 0\n",
    "\n",
    "X_test_all_features = X_test_all_features[X_all_features.columns]\n",
    "\n",
    "\n",
    "X_test_all_features_numeric = X_test_all_features.select_dtypes(include=np.number)\n",
    "X_test_all_features_categorical = X_test_all_features.select_dtypes(exclude=np.number)\n",
    "\n",
    "if not X_test_all_features_numeric.empty:\n",
    "    if 'final_scaler' in locals() and final_scaler is not None:\n",
    "        X_test_scaled_final_numeric = pd.DataFrame(\n",
    "            final_scaler.transform(X_test_all_features_numeric),\n",
    "            columns=X_test_all_features_numeric.columns,\n",
    "            index=X_test_all_features_numeric.index\n",
    "        )\n",
    "        X_test_scaled_final = pd.concat([X_test_scaled_final_numeric, X_test_all_features_categorical], axis=1)\n",
    "        print(\"Финальные тестовые числовые признаки масштабированы StandardScaler.\")\n",
    "    else:\n",
    "        print(\"Предупреждение: final_scaler не был обучен. Пропуск масштабирования тестовых признаков.\")\n",
    "        X_test_scaled_final = X_test_all_features\n",
    "else:\n",
    "    X_test_scaled_final = X_test_all_features_categorical\n",
    "    print(\"Нет числовых признаков для финального масштабирования тестовых данных.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33fbf06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Этап 7: Обучение модели LinearRegression ---\n",
      "Модель LinearRegression обучена.\n",
      "\n",
      "--- Этап 8: Оценка качества модели ---\n",
      "КАЧЕСТВО МОДЕЛИ НА ОБУЧАЮЩИХ ДАННЫХ (train_test_split):\n",
      "R² score: 0.9154\n",
      "MSE: 25.0979\n",
      "RMSE: 5.0098\n",
      "MAE: 3.7249\n",
      "\n",
      "==================================================\n",
      "КАЧЕСТВО МОДЕЛИ НА ВАЛИДАЦИОННЫХ ДАННЫХ (train_test_split):\n",
      "R² score: 0.9087\n",
      "MSE: 27.3001\n",
      "RMSE: 5.2250\n",
      "MAE: 3.8214\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Этап 7: Обучение модели LinearRegression ---\")\n",
    "\n",
    "X_train_final = X_scaled_final\n",
    "y_train_final = y\n",
    "\n",
    "if not X_train_final.empty:\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_final, y_train_final,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Модель LinearRegression обучена.\")\n",
    "    print(\"\\n--- Этап 8: Оценка качества модели ---\")\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    print(\"КАЧЕСТВО МОДЕЛИ НА ОБУЧАЮЩИХ ДАННЫХ (train_test_split):\")\n",
    "    print(f\"R² score: {r2_score(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"MSE: {mean_squared_error(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_train, y_train_pred)):.4f}\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_train, y_train_pred):.4f}\")\n",
    "\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"КАЧЕСТВО МОДЕЛИ НА ВАЛИДАЦИОННЫХ ДАННЫХ (train_test_split):\")\n",
    "    print(f\"R² score: {r2_score(y_val, y_val_pred):.4f}\")\n",
    "    print(f\"MSE: {mean_squared_error(y_val, y_val_pred):.4f}\")\n",
    "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_val, y_val_pred)):.4f}\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_val, y_val_pred):.4f}\")\n",
    "else:\n",
    "    print(\"Ошибка: Обучающие признаки пусты. Модель не может быть обучена.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "648493a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Этап 9: Предсказание и создание submission ---\n",
      "Файл 'predictions.csv' успешно создан.\n",
      "Первые 5 предсказаний:\n",
      "   ID  RiskScore\n",
      "0   0  33.825161\n",
      "1   1  52.997754\n",
      "2   2  30.463817\n",
      "3   3  36.322889\n",
      "4   4  30.825911\n",
      "\n",
      "--- Этап 10: Оценка на тестовых данных (из ex.csv) ---\n",
      "R² score: -0.3435\n",
      "MSE: 1490.8414\n",
      "RMSE: 38.6114\n",
      "MAE: 32.1667\n",
      "\n",
      "--- Работа скрипта завершена ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Этап 9: Предсказание и создание submission ---\")\n",
    "\n",
    "if 'model' in locals() and X_test_scaled_final is not None and not X_test_scaled_final.empty:\n",
    "    y_test_pred = model.predict(X_test_scaled_final)\n",
    "\n",
    "    if test_ids is not None:\n",
    "        submission = pd.DataFrame({'ID': test_ids, 'RiskScore': y_test_pred})\n",
    "        submission.to_csv('predictions.csv', index=False)\n",
    "        print(\"Файл 'predictions.csv' успешно создан.\")\n",
    "        print(\"Первые 5 предсказаний:\")\n",
    "        print(submission.head())\n",
    "    else:\n",
    "        print(\"Предупреждение: Колонка 'ID' не найдена в тестовых данных. Файл submission не создан.\")\n",
    "elif X_test_scaled_final is None or X_test_scaled_final.empty:\n",
    "    print(\"Ошибка: Тестовые признаки пусты или не были подготовлены. Предсказание невозможно.\")\n",
    "else:\n",
    "    print(\"Ошибка: Модель не была обучена. Предсказание невозможно.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"\\n--- Этап 10: Оценка на тестовых данных (из ex.csv) ---\")\n",
    "    ex_csv_path = \"ex.csv\"\n",
    "    df_ex = pd.read_csv(ex_csv_path)\n",
    "\n",
    "    if 'submission' in locals() and not submission.empty:\n",
    "        evaluation_df = pd.merge(df_ex, submission, on='ID', suffixes=('_true', '_pred'))\n",
    "\n",
    "        if 'RiskScore_pred' in evaluation_df.columns and 'RiskScore_true' in evaluation_df.columns:\n",
    "            y_true_eval = evaluation_df['RiskScore_true']\n",
    "            y_pred_eval = evaluation_df['RiskScore_pred']\n",
    "\n",
    "            print(f\"R² score: {r2_score(y_true_eval, y_pred_eval):.4f}\")\n",
    "            print(f\"MSE: {mean_squared_error(y_true_eval, y_pred_eval):.4f}\")\n",
    "            print(f\"RMSE: {np.sqrt(mean_squared_error(y_true_eval, y_pred_eval)):.4f}\")\n",
    "            print(f\"MAE: {mean_absolute_error(y_true_eval, y_pred_eval):.4f}\")\n",
    "        else:\n",
    "            print(\"Не удалось найти колонки 'RiskScore_pred' и 'RiskScore_true' после объединения. Оценка на ex.csv невозможна.\")\n",
    "    else:\n",
    "        print(\"Файл submission не был создан или пуст. Оценка на ex.csv невозможна.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nФайл 'ex.csv' не найден. Оценка на тестовых данных (ex.csv) невозможна.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nПроизошла ошибка при оценке на ex.csv: {e}\")\n",
    "\n",
    "print(\"\\n--- Работа скрипта завершена ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
