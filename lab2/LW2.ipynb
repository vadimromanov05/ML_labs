{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6feafbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    PolynomialFeatures,\n",
    "    OneHotEncoder\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    GridSearchCV\n",
    ")\n",
    "from sklearn.metrics import roc_auc_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e602db69",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"compute.use_numexpr\", False)\n",
    "\n",
    "TRAIN_PATH = \"train_c.csv\"\n",
    "TEST_PATH  = \"test_c.csv\"\n",
    "\n",
    "TARGET_COL = \"LoanApproved\"\n",
    "ID_COL     = \"ID\"       \n",
    "DATE_COL   = \"ApplicationDate\"\n",
    "\n",
    "ABS_CAP           = 1000.0\n",
    "TOP_K_CORR        = 12\n",
    "DEGREE_POLY       = 2\n",
    "SMOOTHING_K       = 3.0\n",
    "MIN_FREQ_OHE       = 0.01\n",
    "MISSING_frac_DROP  = 0.8\n",
    "N_SPLITS_CV        = 5\n",
    "RANDOM_STATE       = 42\n",
    "\n",
    "def parse_number(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    s = str(val).strip().replace(\"\\u00a0\", \"\").replace(\" \", \"\")\n",
    "    if \",\" in s and \".\" in s:\n",
    "        if s.rfind(\",\") > s.rfind(\".\"):\n",
    "            s= s.replace(\".\", \"\").replace(\",\", \".\")\n",
    "        else:\n",
    "            s = s.replace(\",\", \"\")\n",
    "    elif \",\" in s and \".\" not in s:\n",
    "        if s.count(\",\") == 1 and len(s.split(\",\")[-1]) <= 2:\n",
    "            s = s.replace(\",\", \".\")\n",
    "        else:\n",
    "            s = s.replace(\",\", \"\")\n",
    "    s = re.sub(r\"[^0-9eE\\+\\-\\.]\", \"\", s)\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def convert_numeric_like_columns(df, min_fraction_numeric: float = 0.9):\n",
    "    \"\"\"Преобразует столбцы‑строки, которые могут быть числами.\"\"\"\n",
    "    df = df.copy()\n",
    "    numeric_cols = []\n",
    "    for col in df.columns:\n",
    "        s = df[col]\n",
    "        if is_numeric_dtype(s):\n",
    "            numeric_cols.append(col)\n",
    "            continue\n",
    "        sample = s.dropna().astype(str).head(500)\n",
    "        if sample.empty:\n",
    "            continue\n",
    "        parsed = sample.apply(parse_number)\n",
    "        if np.isfinite(parsed).mean() >= min_fraction_numeric:\n",
    "            df[col] = s.apply(parse_number).astype(float)\n",
    "            numeric_cols.append(col)\n",
    "    return df, numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4f75b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(TRAIN_PATH)\n",
    "test      = pd.read_csv(TEST_PATH)\n",
    "\n",
    "y_raw = train_raw[TARGET_COL].apply(parse_number).astype(float)\n",
    "y_filtered = y_raw[np.isfinite(y_raw)]\n",
    "if ABS_CAP is not None:\n",
    "    y_filtered = y_filtered[y_filtered.abs() <= ABS_CAP]\n",
    "\n",
    "train = train_raw.loc[y_filtered.index].copy()\n",
    "train[TARGET_COL] = y_filtered\n",
    "\n",
    "for df_ in (train, test):\n",
    "    if DATE_COL in df_.columns:\n",
    "        df_[DATE_COL] = pd.to_datetime(df_[DATE_COL], errors=\"coerce\")\n",
    "        df_[DATE_COL + \"_year\"]  = df_[DATE_COL].dt.year\n",
    "        df_[DATE_COL + \"_month\"] = df_[DATE_COL].dt.month\n",
    "        df_[DATE_COL + \"_day\"]   = df_[DATE_COL].dt.day\n",
    "        df_.drop(columns=[DATE_COL], inplace=True)\n",
    "\n",
    "feature_cols = [c for c in train.columns if c not in [TARGET_COL, ID_COL]]\n",
    "X_raw  = train[feature_cols].copy()\n",
    "y      = y_filtered.values.astype(float)\n",
    "X_test_raw = test[feature_cols].copy()\n",
    "\n",
    "full_data = pd.concat([X_raw, X_test_raw], axis=0, ignore_index=True)\n",
    "full_converted, numeric_cols_initial = convert_numeric_like_columns(full_data)\n",
    "full_processed = full_converted.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "numeric_cols  = list(numeric_cols_initial)\n",
    "cat_cols_initial = [c for c in full_processed.columns\n",
    "                    if c not in numeric_cols and c not in [TARGET_COL, ID_COL]]\n",
    "\n",
    "skew = full_processed[numeric_cols].skew(numeric_only=True)\n",
    "skewed_cols = skew[skew.abs() > 1].index.tolist()\n",
    "log_cols = []\n",
    "for col in skewed_cols:\n",
    "    if col in numeric_cols and col in full_processed.columns:\n",
    "        s = full_processed[col]\n",
    "        if (s >= 0).mean() > 0.99:\n",
    "            new_col = col + \"_log1p\"\n",
    "            full_processed[new_col] = np.log1p(s.clip(lower=0))\n",
    "            log_cols.append(new_col)\n",
    "            numeric_cols.append(new_col)\n",
    "numeric_cols = [c for c in numeric_cols if c in full_processed.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8c89563",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_frac = full_processed.isna().mean()\n",
    "nunique     = full_processed.nunique(dropna=False)\n",
    "cols_to_drop = list(\n",
    "    missing_frac[missing_frac > MISSING_frac_DROP].index.union(\n",
    "        nunique[nunique <= 1].index\n",
    "    )\n",
    ")\n",
    "if cols_to_drop:\n",
    "    full_processed = full_processed.drop(columns=cols_to_drop)\n",
    "    numeric_cols = [c for c in numeric_cols if c not in cols_to_drop]\n",
    "    cat_cols_initial = [c for c in cat_cols_initial if c not in cols_to_drop]\n",
    "\n",
    "winsorized_numeric_cols = [c for c in numeric_cols if c in full_processed.columns]\n",
    "if winsorized_numeric_cols:\n",
    "    lower_q, upper_q = 0.01, 0.99\n",
    "    lower_bounds = full_processed[winsorized_numeric_cols].quantile(lower_q)\n",
    "    upper_bounds = full_processed[winsorized_numeric_cols].quantile(upper_q)\n",
    "    X_winsorized_part = full_processed[winsorized_numeric_cols].copy()\n",
    "\n",
    "    for col in X_winsorized_part.columns:\n",
    "        x = X_winsorized_part[col].to_numpy(dtype=float, copy=True)\n",
    "        lo = lower_bounds.get(col, np.nan)\n",
    "        hi = upper_bounds.get(col, np.nan)\n",
    "        if np.isfinite(lo):\n",
    "            x = np.where(np.isfinite(x), np.maximum(x, lo), x)\n",
    "        if np.isfinite(hi):\n",
    "            x = np.where(np.isfinite(x), np.minimum(x, hi), x)\n",
    "        X_winsorized_part[col] = x\n",
    "    full_processed[winsorized_numeric_cols] = X_winsorized_part\n",
    "\n",
    "te_cols = []\n",
    "y_series_for_te = pd.Series(y, index=range(len(X_raw)))\n",
    "cat_cols_current = [c for c in full_processed.columns\n",
    "                    if c not in numeric_cols and c not in [TARGET_COL, ID_COL]]\n",
    "\n",
    "for col in cat_cols_current:\n",
    "    if col in full_processed.columns:\n",
    "        train_vals = full_processed[col].iloc[:len(X_raw)].astype(str)\n",
    "        stats = y_series_for_te.groupby(train_vals).agg([\"mean\", \"count\"])\n",
    "        global_mean = y_series_for_te.mean()\n",
    "        smooth = (stats[\"mean\"] * stats[\"count\"] + SMOOTHING_K * global_mean) / (\n",
    "            stats[\"count\"] + SMOOTHING_K\n",
    "        )\n",
    "        te_feature_name = col + \"_te\"\n",
    "        full_processed[te_feature_name] = (\n",
    "            full_processed[col].astype(str).map(smooth).fillna(global_mean)\n",
    "        )\n",
    "        te_cols.append(te_feature_name)\n",
    "        numeric_cols.append(te_feature_name)\n",
    "\n",
    "numeric_corr = {}\n",
    "base_num_cols_for_corr = [\n",
    "    c for c in numeric_cols\n",
    "    if not c.endswith((\"_te\", \"_log1p\", \"_bin\")) and c in X_raw.columns\n",
    "]\n",
    "for col in base_num_cols_for_corr:\n",
    "    s = full_processed[col].iloc[:len(X_raw)]\n",
    "    mask = np.isfinite(s.values) & np.isfinite(y)\n",
    "    if mask.sum() < 50:\n",
    "        continue\n",
    "    try:\n",
    "        c = np.corrcoef(s.values[mask], y[mask])[0, 1]\n",
    "        if np.isfinite(c):\n",
    "            numeric_corr[col] = abs(c)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "numeric_corr_sorted = sorted(numeric_corr.items(),\n",
    "                            key=lambda x: x[1], reverse=True)\n",
    "top_poly_bin_cols_candidates = [c for c, _ in numeric_corr_sorted[:TOP_K_CORR]]\n",
    "\n",
    "top_bin_cols = []\n",
    "for col in top_poly_bin_cols_candidates:\n",
    "    if col in numeric_cols and col in full_processed.columns:\n",
    "        s_train = full_processed[col].iloc[:len(X_raw)]\n",
    "        mask = np.isfinite(s_train.values)\n",
    "        if mask.sum() < 100:\n",
    "            continue\n",
    "        valid_s = s_train[mask]\n",
    "        try:\n",
    "            num_bins = min(10, len(valid_s) // 5)\n",
    "            if num_bins < 2:\n",
    "                continue\n",
    "            _, bin_edges = pd.qcut(valid_s, q=num_bins, retbins=True, duplicates=\"drop\")\n",
    "            if len(bin_edges) < 2:\n",
    "                continue\n",
    "            bin_col_name = col + \"_bin\"\n",
    "            full_processed[bin_col_name] = pd.cut(\n",
    "                full_processed[col], bins=bin_edges, include_lowest=True, duplicates=\"drop\"\n",
    "            )\n",
    "            cat_cols_current.append(bin_col_name)\n",
    "            top_bin_cols.append(bin_col_name)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf19c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_transformer = PolynomialFeatures(degree=DEGREE_POLY, include_bias=False)\n",
    "poly_cols_to_transform = [\n",
    "    c for c in top_poly_bin_cols_candidates\n",
    "    if c in numeric_cols and c in full_processed.columns\n",
    "]\n",
    "poly_feature_names = []\n",
    "if poly_cols_to_transform:\n",
    "    poly_processing_pipeline = Pipeline(\n",
    "        [(\"imputer\", SimpleImputer(strategy=\"median\")), (\"poly\", poly_transformer)]\n",
    "    )\n",
    "    poly_features_train = poly_processing_pipeline.fit(\n",
    "        full_processed[poly_cols_to_transform].iloc[:len(X_raw)]\n",
    "    ).transform(\n",
    "        full_processed[poly_cols_to_transform].iloc[:len(X_raw)]\n",
    "    )\n",
    "    poly_feature_names = poly_transformer.get_feature_names_out(poly_cols_to_transform)\n",
    "\n",
    "    linear_col_set = set(poly_cols_to_transform)\n",
    "    keep_mask = [name not in linear_col_set for name in poly_feature_names]\n",
    "    poly_features_train = poly_features_train[:, keep_mask]\n",
    "    poly_feature_names = np.array(poly_feature_names)[keep_mask]\n",
    "\n",
    "    poly_df = pd.DataFrame(\n",
    "        poly_features_train,\n",
    "        index=full_processed.iloc[:len(X_raw)].index,\n",
    "        columns=poly_feature_names,\n",
    "    )\n",
    "\n",
    "    poly_features_test = poly_processing_pipeline.transform(\n",
    "        full_processed[poly_cols_to_transform].iloc[len(X_raw):]\n",
    "    )\n",
    "    poly_features_test = poly_features_test[:, keep_mask]\n",
    "    poly_df_test = pd.DataFrame(\n",
    "        poly_features_test,\n",
    "        index=full_processed.iloc[len(X_raw):].index,\n",
    "        columns=poly_feature_names,\n",
    "    )\n",
    "\n",
    "    poly_df_full = pd.concat([poly_df, poly_df_test], axis=0).sort_index()\n",
    "\n",
    "    full_processed = pd.concat([full_processed, poly_df_full], axis=1)\n",
    "    numeric_cols.extend(poly_feature_names)\n",
    "\n",
    "numeric_cols = [c for c in numeric_cols if c in full_processed.columns]\n",
    "cat_cols_current = [c for c in cat_cols_current if c in full_processed.columns]\n",
    "X   = full_processed.iloc[:len(X_raw)].reset_index(drop=True)\n",
    "X_test = full_processed.iloc[len(X_raw):].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3492313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Размеры:\n",
      "train    (8389, 136) (8389,)\n",
      "validation (2098, 136) (2098,)\n",
      "\n",
      "Распределение валидации:\n",
      "1.0    0.511916\n",
      "0.0    0.488084\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "numeric_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scale\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", min_frequency=MIN_FREQ_OHE)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", numeric_pipeline, numeric_cols),\n",
    "        (\"cat\", categorical_pipeline, cat_cols_current),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nРазмеры:\")\n",
    "print(f\"train    {X_train.shape} {y_train.shape}\")\n",
    "print(f\"validation {X_valid.shape} {y_valid.shape}\")\n",
    "print(\"\\nРаспределение валидации:\")\n",
    "print(pd.Series(y_valid).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64de4e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GridSearchLR ===\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\n",
      "Лучшие параметры: {'clf__C': 0.01, 'clf__class_weight': 'balanced'}\n",
      "Лучший ROC‑AUC CV: 0.98511344836377\n",
      "\n",
      "LR ROC‑AUC на vaild: 0.9814334919110801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.93      0.92      1024\n",
      "         1.0       0.93      0.92      0.93      1074\n",
      "\n",
      "    accuracy                           0.93      2098\n",
      "   macro avg       0.93      0.93      0.93      2098\n",
      "weighted avg       0.93      0.93      0.93      2098\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg_pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\n",
    "            \"clf\",\n",
    "            LogisticRegression(\n",
    "                max_iter=2000,\n",
    "                tol=1e-4,\n",
    "                solver=\"saga\",\n",
    "                penalty=\"l2\",\n",
    "                n_jobs=4,\n",
    "                random_state=RANDOM_STATE,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "sgd_pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\n",
    "            \"clf\",\n",
    "            SGDClassifier(\n",
    "                loss=\"log_loss\",\n",
    "                penalty=\"elasticnet\",\n",
    "                alpha=0.0005,\n",
    "                l1_ratio=0.15,\n",
    "                max_iter=1000,\n",
    "                tol=1e-4,\n",
    "                n_jobs=4,\n",
    "                random_state=RANDOM_STATE,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "svc_pipe_base = Pipeline(\n",
    "    [\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\n",
    "            \"clf\",\n",
    "            LinearSVC(\n",
    "                C=1.0,\n",
    "                max_iter=10000,\n",
    "                tol=1e-5,\n",
    "                random_state=RANDOM_STATE,\n",
    "                dual=False,          # <‑‑ ключ\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "svc_pipe = CalibratedClassifierCV(\n",
    "    svc_pipe_base, cv=3, method=\"sigmoid\", n_jobs=4\n",
    ")\n",
    "\n",
    "param_grid_lr = {\n",
    "    \"clf__C\": [0.01, 0.1, 1.0],\n",
    "    \"clf__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "print(\"\\n=== GridSearchLR ===\")\n",
    "grid_search_lr = GridSearchCV(\n",
    "    estimator=log_reg_pipe,\n",
    "    param_grid=param_grid_lr,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=StratifiedKFold(n_splits=N_SPLITS_CV, shuffle=True, random_state=RANDOM_STATE),\n",
    "    n_jobs=1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nЛучшие параметры:\", grid_search_lr.best_params_)\n",
    "print(\"Лучший ROC‑AUC CV:\", grid_search_lr.best_score_)\n",
    "\n",
    "best_lr = grid_search_lr.best_estimator_\n",
    "y_valid_proba_lr = best_lr.predict_proba(X_valid)[:, 1]\n",
    "auc_lr = roc_auc_score(y_valid, y_valid_proba_lr)\n",
    "print(\"\\nLR ROC‑AUC на vaild:\", auc_lr)\n",
    "print(classification_report(y_valid, best_lr.predict(X_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f7a44ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Обучаем SGD ===\n",
      "SGD ROC‑AUC: 0.9811407050162941\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.93      0.92      1024\n",
      "         1.0       0.93      0.92      0.93      1074\n",
      "\n",
      "    accuracy                           0.93      2098\n",
      "   macro avg       0.93      0.93      0.93      2098\n",
      "weighted avg       0.93      0.93      0.93      2098\n",
      "\n",
      "\n",
      "=== Обучаем SVC (калибровка) ===\n",
      "SVC ROC‑AUC: 0.9818108414804471\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.93      0.93      1024\n",
      "         1.0       0.93      0.92      0.93      1074\n",
      "\n",
      "    accuracy                           0.93      2098\n",
      "   macro avg       0.93      0.93      0.93      2098\n",
      "weighted avg       0.93      0.93      0.93      2098\n",
      "\n",
      "\n",
      "=== Ансамбль ROC‑AUC === 0.9824727944599628\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.93      0.92      1024\n",
      "         1.0       0.93      0.92      0.93      1074\n",
      "\n",
      "    accuracy                           0.93      2098\n",
      "   macro avg       0.93      0.93      0.93      2098\n",
      "weighted avg       0.93      0.93      0.93      2098\n",
      "\n",
      "\n",
      "=== Финальное обучение на всей данных и предсказание теста ===\n",
      "\n",
      "Файл submission.csv сохранён\n",
      "   ID  LoanApproved\n",
      "0   0      0.983798\n",
      "1   1      0.013294\n",
      "2   2      0.998884\n",
      "3   3      0.998308\n",
      "4   4      0.999427\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Обучаем SGD ===\")\n",
    "sgd_pipe.fit(X_train, y_train)\n",
    "y_valid_proba_sgd = sgd_pipe.predict_proba(X_valid)[:, 1]\n",
    "auc_sgd = roc_auc_score(y_valid, y_valid_proba_sgd)\n",
    "print(\"SGD ROC‑AUC:\", auc_sgd)\n",
    "print(classification_report(y_valid, sgd_pipe.predict(X_valid)))\n",
    "\n",
    "print(\"\\n=== Обучаем SVC (калибровка) ===\")\n",
    "svc_pipe.fit(X_train, y_train)\n",
    "y_valid_proba_svc = svc_pipe.predict_proba(X_valid)[:, 1]\n",
    "auc_svc = roc_auc_score(y_valid, y_valid_proba_svc)\n",
    "print(\"SVC ROC‑AUC:\", auc_svc)\n",
    "print(classification_report(y_valid, svc_pipe.predict(X_valid)))\n",
    "\n",
    "ensemble_valid_proba = (y_valid_proba_lr + y_valid_proba_sgd + y_valid_proba_svc) / 3\n",
    "auc_ensemble = roc_auc_score(y_valid, ensemble_valid_proba)\n",
    "print(\"\\n=== Ансамбль ROC‑AUC ===\", auc_ensemble)\n",
    "print(classification_report(y_valid, (ensemble_valid_proba >= 0.5).astype(int)))\n",
    "\n",
    "print(\"\\n=== Финальное обучение на всей данных и предсказание теста ===\")\n",
    "best_lr.fit(X, y)\n",
    "sgd_pipe.fit(X, y)\n",
    "svc_pipe.fit(X, y)\n",
    "\n",
    "test_p1 = best_lr.predict_proba(X_test)[:, 1]\n",
    "test_p2 = sgd_pipe.predict_proba(X_test)[:, 1]\n",
    "test_p3 = svc_pipe.predict_proba(X_test)[:, 1]\n",
    "test_proba = (test_p1 + test_p2 + test_p3) / 3\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test[ID_COL] if ID_COL is not None else np.arange(len(test_proba)),\n",
    "    TARGET_COL: test_proba,\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nФайл submission.csv сохранён\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
